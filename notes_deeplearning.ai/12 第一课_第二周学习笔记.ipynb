{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 第一课_第二周学习笔记\n",
    "---\n",
    "\n",
    ">在第一课第二周的的课程中，ng以识别图片中是否有猫为例，简单介绍了一下二分类（Binary Classification）问题，以此引出对数几率回归算法(Logistic Regression)。Logistic Regression 是一种常用的二分类问题求解算法，通常用于处理监督学习（supervised learning）相关问题。在讲解LR（Logistic Regression）算法的过程中，ng穿插着介绍了梯度下降（Gradient Descent）、导数（derivatives）、计算图（computation graph）和向量化（Vectorization）。此外，ng还特意介绍了Python中的广播（Boradcasting）机制，一种有效提高计算效率的方法，也可以认为是向量化在python中的实现。\n",
    "\n",
    "在这份笔记中，我将先记录导数（derivatives）、计算图（computation graph）、向量化（Vectorization）等基本概念，然后记录梯度下降（Gradient Descent）相关知识，最后记录Logistic Regression算法及案例 — 识别图片中是否有猫。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 名词约定\n",
    "----\n",
    "|名词|含义|\n",
    "|:---:|:---|\n",
    "|ng|Andrew Ng, 吴恩达老师|\n",
    "|数据集|所有个案构成的集合|\n",
    "|样本|数据集中的某一个案|\n",
    "|特征|样本中的变量|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - 导数（derivative）基础\n",
    "---\n",
    "直观上理解，导数的意义是计算函数 $f(x)$ 在 $x_0$ 处的斜率，即$y=f(x)$的变化速率。\n",
    "导数的定义如下：\n",
    "<img src=\"images/dd1.jpg\" style=\"width:600px\">\n",
    "\n",
    "---\n",
    "基本求导法则：\n",
    "<img src=\"images/dc1.jpg\" style=\"width:600px\">\n",
    "\n",
    "---\n",
    "函数的和、差、积、商的求导法则：\n",
    "<img src=\"images/dc2.jpg\" style=\"width:600px\">\n",
    "\n",
    "---\n",
    "复合函数的求导法则：\n",
    "<img src=\"images/dc4.jpg\" style=\"width:600px\">\n",
    "\n",
    "*注：以上图片来自于同济大学编写的《高等数学（第六版）》。*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - 计算图（computation graph）\n",
    "---\n",
    "computatiom graph 是用有向箭头表示数据流动方向、方框表示计算内容的计算流程图，能够很好的描述算法的整个计算过程，对于理解算法很有帮助。ng给出的一个案例如下：\n",
    "<img src=\"images/cg1.jpg\" style=\"width:520px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - 向量化（vectorization）\n",
    "---\n",
    "\n",
    "向量化是提高计算效率的有效方法，其核心思想是将需要循环的累加、累乘等计算任务，通过合适的数据转换（如：转置、广播、堆叠等），变成矩阵或向量计算任务。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x1 = np.linspace(1, 100, 10000)\n",
    "x2 = np.linspace(1, 10, 10000)\n",
    "\n",
    "# 假设要计算向量x1与x2的乘积和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeitResult : 100 loops, best of 3: 5.26 ms per loop>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用循环进行计算\n",
    "%timeit -qo sum([x1[i]*x2[i] for i in range(x1.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeitResult : 100000 loops, best of 3: 4.19 µs per loop>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用向量化方法进行计算\n",
    "%timeit -qo np.dot(x1, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比较以上两个计算耗时，可以发现：向量化计算x1与x2的乘积和比循环的方法块1000多倍！！！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - 梯度下降法（Gradient Descent）\n",
    "---\n",
    "梯度下降法（Gradient Descent）是一个最优化算法，通常也称为最速下降法，它将负梯度方向作为搜索方向，越接近最优化目标，步长越小，前进越慢，当步长小于一个给定的值或迭代次数达到设定的上限，迭代停止。\n",
    "\n",
    "对于任一参数 $\\theta$，使用梯度下降法的参数值更新公式为： $$ \\theta = \\theta - \\alpha \\text{ } d\\theta$$其中 $\\alpha$ 是学习率（learning rate）。\n",
    "\n",
    "如果你想了解更多关于梯度下降法的知识，请点击 [wiki - Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - 随机梯度下降法（Stochastic Gradient Descent）\n",
    "---\n",
    "SGD的优势在于，每一次迭代中随机选取一个样本计算梯度，而不是使用全部样本。\n",
    "\n",
    "SGD不需要记录哪些样例已经在前面的迭代过程中被访问过，有时候随机梯度下降能够直接优化期望风险，因为样例可能是随机从真正的分布中选取的。 \n",
    "\n",
    "如果你想了解更多关于随机梯度下降法的知识，请点击 [wiki - Stochastic Gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - 对数几率回归（Logistic Regression）\n",
    "---\n",
    "事实上，Logistic Regression是一个简单的神经网络。由于 LR 本质上是一个线性回归模型，因此 LR 主要适用于线性可分（linearly separable）的二分类问题，也可以应用于线性可分的多分类问题([案例](http://blog.csdn.net/bdss58/article/details/42065383))。\n",
    "\n",
    "ng在讲解Logistic Regression算法时，以图像中是否有猫这个经典的二分类问题为例，将整个算法流程做了介绍，并在[作业](https://github.com/zengbin93/blog/blob/master/notes_deeplearning.ai/homework/012%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_%E7%AC%AC%E4%BA%8C%E5%91%A8_%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/assignment2_2.ipynb)中引导学习者实现了这一算法。\n",
    "\n",
    "使用训练好的LR模型判断图片中是否有猫的过程如下图：\n",
    "<img src=\"images/LogReg_kiank.png\" style=\"width:400px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - 符号约定\n",
    "---\n",
    "\n",
    "|符号|含义|\n",
    "|:-----:|:----|\n",
    "|$X$|堆叠（stack）所有输入样本得到的输入矩阵|\n",
    "|$x^{(i)}$|数据集中的第 $i$ 个样本的输入向量|\n",
    "|$Y$|堆叠（stack）之后的真实值（标签）向量|\n",
    "|$y^{(i)}$|第 $i$ 个样本的真实值（标签）|\n",
    "|$\\hat{y}^{(i)}$|第 $i$ 个样本的最终预测值|\n",
    "|$a^{(i)}$|训练过程中，第 $i$ 个样本的预测值|\n",
    "|$A$|训练过程中，所有样本的预测值矩阵|\n",
    "|$m$|输入样本数量|\n",
    "|$n$|每一个样本的特征数量|\n",
    "|$w_i$|第 $i$ 个特征的权重系数|\n",
    "|$W$|所有特征的权重系数向量|\n",
    "|$b$|线性回归中的截断误差（偏置项）|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - 基本概念\n",
    "---\n",
    ">1. Sigmoid函数\n",
    ">2. 损失函数（loss function）\n",
    ">3. 成本函数（cost function）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 - Sigmoid函数\n",
    "---\n",
    "sigmoid函数是一个非线性函数，在机器学习和深度学习中都有应用，有时也称作对数几率函数（logistic function）。\n",
    "\n",
    "sigmoid函数表达式：\n",
    "$$sigmoid(x) = \\sigma(x)  = \\frac{1}{1+e^{-x}}$$ \n",
    "\n",
    "sigmoid函数的导数：\n",
    "$$sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))$$\n",
    "\n",
    "---\n",
    "sigmoid函数及其导数的Python实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"  \n",
    "    s = 1.0 / (1.0 + np.exp(-x)) \n",
    "    return s\n",
    "\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    Compute the derivative of the sigmoid function with respect to its input x.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array\n",
    "\n",
    "    Return:\n",
    "    ds -- Your computed gradient.\n",
    "    \"\"\"\n",
    "    s = sigmoid(x)\n",
    "    ds = s * (1 - s)    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x19318801358>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFpCAYAAACmt+D8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1931781ecf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-10, 10, 10000)\n",
    "y_sigmoid = sigmoid(x)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(x, y_sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 - 损失函数（loss function）\n",
    "---\n",
    "损失函数（loss function）用于计算单一样本的偏差，LR中的loss function如下：\n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})$$\n",
    "其中，$a^{(i)}$ 表示第 $i$ 个样本的预测值，$y^{(i)}$ 表示第 $i$ 个样本的真实值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 - 成本函数（cost function）\n",
    "---\n",
    "成本函数（cost function）用于评估整个数据集的偏差，计算公式如下：\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})$$\n",
    "直观的理解，成本函数计算的是数据集中所有样本偏差的均值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 - LR模型训练过程\n",
    "---\n",
    ">step 1. 初始化参数 $W$ 和 $b$\n",
    "\n",
    ">step 2. 计算当前参数对应的cost（成本）和gradient（梯度）\n",
    "\n",
    ">step 3. 使用梯度下降法更新参数，直到迭代停止\n",
    "\n",
    ">step 4. 得到训练好的参数，即LR模型训练完成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 - 初始化参数\n",
    "---\n",
    "LR算法中的初始化参数有两个，分别是 $W$ 和 $b$，通常设定 $W$ 为长度为 $n$ 的零向量（即 $W=np.zeros((n, 1))$）， $b=0$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 - 计算当前参数对应的cost（成本）和gradient（梯度）\n",
    "---\n",
    "\n",
    "对于给定的参数 $W$ 和 $b$，其在当前数据集堆叠（stack）得到的输入矩阵 $X$ 上的cost计算过程如下：\n",
    "\n",
    "---\n",
    "1) 计算当前参数对应的预测值：\n",
    "$$A = \\sigma(w^T X + b) = (a^{(0)}, a^{(1)}, ..., a^{(m-1)}, a^{(m)})$$\n",
    "2) 根据预测值和真实值，计算cost：\n",
    "$$J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$$\n",
    "\n",
    "---\n",
    "gradient 计算过程如下：\n",
    "\n",
    "---\n",
    "由于LR中有两个参数，因此需要分别计算它们的梯度(导数):\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 使用梯度下降法更新参数\n",
    "---\n",
    "使用梯度下降法更新参数的目标是最小化 cost function $J$，过程见下图：\n",
    "\n",
    "<img src=\"images\\gradient_descent.jpg\" style=\"width:300px\">\n",
    "\n",
    "为了得到cost“最小”时对应的参数 $W$ 和 $b$，需要进行迭代计算。考虑到迭代存在不收敛的情况，引入num_iterations参数，当迭代次数大于num_iterations时，迭代停止，输出结果。此外，为了防止gradient过大引起学习曲线（learning curve）上下跳动，引入learning_rate参数，控制参数更新速度。\n",
    "\n",
    "---\n",
    "在梯度下降法中，对于任一参数 $\\theta$ 的参数值更新公式为： $$ \\theta = \\theta - \\alpha \\text{ } d\\theta$$其中 $\\alpha$ 是学习率（learning rate）。\n",
    "\n",
    "因此，LR中基于梯度下降法的参数更新公式如下：\n",
    "$$W = W - \\alpha * \\frac{\\partial J}{\\partial w}$$\n",
    "$$b = b - \\alpha * \\frac{\\partial J}{\\partial b}$$\n",
    "\n",
    "每一次参数更新之后，重新计算cost和gradient。循环，直到迭代停止。\n",
    "\n",
    "---\n",
    "满足以下任一条件，迭代停止：\n",
    "1. 迭代次数超过num_iterations\n",
    "2. 参数更新步长（梯度）小于一个给定的值\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4 输出训练好的参数，得到LR模型\n",
    "---\n",
    "使用梯度下降法更新参数完成之后，可以得到最终的 $W$ 和 $b$，即LR模型确定。接下来，就可以使用这个模型去进行新样本的预测了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他资源\n",
    "---\n",
    "1. [broadcasting - numpy](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
    "2. [Coursera吴恩达《神经网络与深度学习》课程笔记（2）-- 神经网络基础之逻辑回归](http://blog.csdn.net/red_stone1/article/details/77851177)\n",
    "3. [吴恩达Coursera Deep Learning学习笔记 1 （下）](http://www.jianshu.com/p/51a5ff911c41)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
